{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkj24WGBBEqZRet6fN2Peq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naidk/Internship-Portfolio/blob/main/Data-Science/Pima_Diabetes_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Initial Setup"
      ],
      "metadata": {
        "id": "RUH5EXGju-b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(\"scikit-learn version:\", sklearn.__version__)\n"
      ],
      "metadata": {
        "id": "WNQIs33Cw1mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "cmgwLmaYu_M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('diabetes.csv')"
      ],
      "metadata": {
        "id": "2Esh6S-cvkYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "qWFL_Scjv17S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()\n",
        "df.info()\n",
        "sns.boxplot(data=df)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Boxplot for Outlier Detection\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_eMNDCSTv21m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Full EDA + Data Cleaning Tasks"
      ],
      "metadata": {
        "id": "gfoEsTOcweJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"diabetes.csv\")  # Or replace with your actual path\n",
        "\n",
        "# 1. Check for Missing Values\n",
        "print(\"ðŸ”¹ Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 2. Check Data Types\n",
        "print(\"\\nðŸ”¹ Data Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# 3. Basic Descriptive Statistics\n",
        "print(\"\\nðŸ”¹ Descriptive Stats:\")\n",
        "print(df.describe())\n",
        "\n",
        "# 4. Check for Duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"\\nðŸ”¹ Duplicates Found: {duplicates}\")\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# 5. Class Balance Visualization\n",
        "sns.countplot(x='Outcome', data=df)\n",
        "plt.title(\"ðŸ”¹ Class Balance\")\n",
        "plt.show()\n",
        "\n",
        "# 6. Feature Distributions\n",
        "df.hist(figsize=(12, 10), bins=20)\n",
        "plt.suptitle(\"ðŸ”¹ Feature Distributions\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "# 7. Correlation Heatmap\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"ðŸ”¹ Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# 8. Z-Score for Outlier Check\n",
        "z_scores = np.abs(zscore(df.drop('Outcome', axis=1)))\n",
        "outliers = (z_scores > 3).sum(axis=0)\n",
        "print(\"\\nðŸ”¹ Z-Score Based Outlier Count Per Feature:\")\n",
        "print(outliers)\n",
        "\n",
        "# 9. Skewness\n",
        "print(\"\\nðŸ”¹ Skewness of Features:\")\n",
        "print(df.drop('Outcome', axis=1).skew().sort_values(ascending=False))\n"
      ],
      "metadata": {
        "id": "qwcmaqvcwe_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns where 0 is biologically invalid\n",
        "cols_with_zero_as_nan = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
        "\n",
        "# Replace 0s with NaN\n",
        "df[cols_with_zero_as_nan] = df[cols_with_zero_as_nan].replace(0, np.nan)\n"
      ],
      "metadata": {
        "id": "VxWAxO7NyBRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing values after treating 0s as NaN:\")\n",
        "print(df[cols_with_zero_as_nan].isnull().sum())\n"
      ],
      "metadata": {
        "id": "IoDFQvsHyC26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Median imputation\n",
        "df[cols_with_zero_as_nan] = df[cols_with_zero_as_nan].fillna(df[cols_with_zero_as_nan].median())\n"
      ],
      "metadata": {
        "id": "TuqyTC0ryFmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final missing values after imputation:\")\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "k-3LcDLByIGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3A: Outlier Detection Using Z-Score Method"
      ],
      "metadata": {
        "id": "tQglYKQHy9aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "def remove_outliers_zscore(df, threshold=3):\n",
        "    df_clean = df.copy()\n",
        "    z_scores = np.abs(zscore(df_clean.select_dtypes(include=[np.number])))\n",
        "    mask = (z_scores < threshold).all(axis=1)\n",
        "    df_clean = df_clean[mask]\n",
        "    return df_clean\n"
      ],
      "metadata": {
        "id": "LLD6LJ1ry10b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_outliers = remove_outliers_zscore(df)\n",
        "print(\"Shape after outlier removal:\", df_no_outliers.shape)\n"
      ],
      "metadata": {
        "id": "Mjuq9Vuyy8Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_outliers = df.copy()           # Cleaned for zeros but keeps outliers\n",
        "df_without_outliers = df_no_outliers   # Cleaned for zeros AND outliers\n"
      ],
      "metadata": {
        "id": "TwBNWE1HzJRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Log1p Transformation Code (Log(1 + x) to handle 0s)"
      ],
      "metadata": {
        "id": "93vvAz7SyuGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "skewed_features = [\"Insulin\", \"DiabetesPedigreeFunction\", \"Age\", \"Pregnancies\"]\n",
        "\n",
        "for feature in skewed_features:\n",
        "    # Only apply if feature has positive values\n",
        "    df_with_outliers[feature] = np.log1p(df_with_outliers[feature])\n",
        "    df_without_outliers[feature] = np.log1p(df_without_outliers[feature])\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRyBQiksyvJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-check skewness after transformation\n",
        "print(\"ðŸ” Skewness after log1p transformation (with outliers):\")\n",
        "print(df_with_outliers[skewed_features].skew())\n",
        "\n",
        "print(\"\\nðŸ” Skewness after log1p transformation (without outliers):\")\n",
        "print(df_without_outliers[skewed_features].skew())\n"
      ],
      "metadata": {
        "id": "SdXjf-jg0C94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If Outcome column was lost during transformations, reattach it\n",
        "df_with_outliers[\"Outcome\"] = df[\"Outcome\"]\n",
        "df_without_outliers[\"Outcome\"] = df[\"Outcome\"]\n"
      ],
      "metadata": {
        "id": "vb0F0BFZ0rK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Separate features and target\n",
        "X_with_outliers = df_with_outliers.drop(\"Outcome\", axis=1)\n",
        "y_with_outliers = df_with_outliers[\"Outcome\"]\n",
        "\n",
        "X_without_outliers = df_without_outliers.drop(\"Outcome\", axis=1)\n",
        "y_without_outliers = df_without_outliers[\"Outcome\"]\n",
        "\n",
        "# Scale the features\n",
        "scaler_with = StandardScaler()\n",
        "scaler_without = StandardScaler()\n",
        "\n",
        "X_with_outliers_scaled = scaler_with.fit_transform(X_with_outliers)\n",
        "X_without_outliers_scaled = scaler_without.fit_transform(X_without_outliers)\n",
        "\n",
        "print(\"Shapes:\", X_with_outliers_scaled.shape, X_without_outliers_scaled.shape)\n"
      ],
      "metadata": {
        "id": "c4h_cOd-0xXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Train-Test Split (on both datasets)"
      ],
      "metadata": {
        "id": "r9yglUPw1KkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Features & Target\n",
        "X_with = df_with_outliers.drop(\"Outcome\", axis=1)\n",
        "y_with = df_with_outliers[\"Outcome\"]\n",
        "\n",
        "X_without = df_without_outliers.drop(\"Outcome\", axis=1)\n",
        "y_without = df_without_outliers[\"Outcome\"]\n",
        "\n",
        "# Split both\n",
        "X_train_with, X_test_with, y_train_with, y_test_with = train_test_split(X_with, y_with, test_size=0.2, random_state=42)\n",
        "X_train_wo, X_test_wo, y_train_wo, y_test_wo = train_test_split(X_without, y_without, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "ITiP1DKt1HYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”„ Step: Handle Class Imbalance using SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# SMOTE for 'with outliers' data\n",
        "smote_with = SMOTE(random_state=42)\n",
        "X_train_with_smote, y_train_with_smote = smote_with.fit_resample(X_train_with, y_train_with)\n",
        "\n",
        "# SMOTE for 'without outliers' data\n",
        "smote_wo = SMOTE(random_state=42)\n",
        "X_train_wo_smote, y_train_wo_smote = smote_wo.fit_resample(X_train_wo, y_train_wo)\n",
        "\n",
        "# Check class distribution after SMOTE\n",
        "print(\"ðŸ”¹ Class Distribution After SMOTE (With Outliers):\")\n",
        "print(y_train_with_smote.value_counts())\n",
        "print(\"\\nðŸ”¹ Class Distribution After SMOTE (Without Outliers):\")\n",
        "print(y_train_wo_smote.value_counts())\n"
      ],
      "metadata": {
        "id": "Ie2RvfgXJDCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Standard Scaling"
      ],
      "metadata": {
        "id": "dkDbdpP-1RKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create scaler objects\n",
        "scaler_with = StandardScaler()\n",
        "scaler_wo = StandardScaler()\n",
        "\n",
        "# Scale after SMOTE\n",
        "X_train_with_scaled = scaler_with.fit_transform(X_train_with_smote)\n",
        "X_test_with_scaled = scaler_with.transform(X_test_with)  # use original test set\n",
        "\n",
        "X_train_wo_scaled = scaler_wo.fit_transform(X_train_wo_smote)\n",
        "X_test_wo_scaled = scaler_wo.transform(X_test_wo)        # use original test set\n"
      ],
      "metadata": {
        "id": "1kgaxa3W1RyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 8: Model Training (Start with Logistic Regression)"
      ],
      "metadata": {
        "id": "WmrVlgcD1ak-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Train using SMOTE-balanced data\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_with_scaled, y_train_with_smote)\n",
        "\n",
        "# Predict on the original (unchanged) test set\n",
        "y_pred_with = lr.predict(X_test_with_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"ðŸ”¹ Logistic Regression (With Outliers + SMOTE):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_with, y_pred_with))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_with, y_pred_with))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_with, y_pred_with))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_with, lr.predict_proba(X_test_with_scaled)[:, 1]))\n"
      ],
      "metadata": {
        "id": "nSJFAgOR1bZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Train using SMOTE-balanced data\n",
        "lr_wo = LogisticRegression()\n",
        "lr_wo.fit(X_train_wo_scaled, y_train_wo_smote)\n",
        "\n",
        "# Predict on the original (unchanged) test set\n",
        "y_pred_wo = lr_wo.predict(X_test_wo_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"ðŸ”¹ Logistic Regression (Without Outliers + SMOTE):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_wo))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_wo, y_pred_wo))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_wo))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_wo, lr_wo.predict_proba(X_test_wo_scaled)[:, 1]))\n"
      ],
      "metadata": {
        "id": "_rujsbQU1oUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "\n",
        "# Train on SMOTE-balanced data\n",
        "rf_with = RandomForestClassifier(random_state=42)\n",
        "rf_with.fit(X_train_with_scaled, y_train_with_smote)\n",
        "\n",
        "# Predict on the original test set\n",
        "y_pred_with_rf = rf_with.predict(X_test_with_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"ðŸ”¹ Random Forest (With Outliers + SMOTE):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_with, y_pred_with_rf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_with, y_pred_with_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_with, y_pred_with_rf))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_with, rf_with.predict_proba(X_test_with_scaled)[:, 1]))\n"
      ],
      "metadata": {
        "id": "0N7BbiyJ9zAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "\n",
        "# Model\n",
        "rf_wo = RandomForestClassifier(random_state=42)\n",
        "rf_wo.fit(X_train_wo_scaled, y_train_wo_smote)\n",
        "\n",
        "# Predict on the original test set\n",
        "y_pred_wo_rf = rf_wo.predict(X_test_wo_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nðŸ”¹ Random Forest (Without Outliers + SMOTE):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_wo_rf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_wo, y_pred_wo_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_wo_rf))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_wo, rf_wo.predict_proba(X_test_wo_scaled)[:, 1]))\n"
      ],
      "metadata": {
        "id": "7-u9RvzF929U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "\n",
        "xgb_with = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_with.fit(X_train_with_scaled, y_train_with_smote)\n",
        "y_pred_xgb_with = xgb_with.predict(X_test_with_scaled)\n",
        "\n",
        "print(\"ðŸ”¹ XGBoost (With Outliers + SMOTE):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_with, y_pred_xgb_with))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_with, y_pred_xgb_with))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_with, y_pred_xgb_with))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_with, xgb_with.predict_proba(X_test_with_scaled)[:, 1]))\n"
      ],
      "metadata": {
        "id": "_TKhzHlu_my2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_wo = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_wo.fit(X_train_wo_scaled, y_train_wo_smote)\n",
        "y_pred_xgb_wo = xgb_wo.predict(X_test_wo_scaled)\n",
        "\n",
        "print(\"\\nðŸ”¹ XGBoost (Without Outliers + SMOTE):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_xgb_wo))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_wo, y_pred_xgb_wo))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_xgb_wo))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_wo, xgb_wo.predict_proba(X_test_wo_scaled)[:, 1]))\n"
      ],
      "metadata": {
        "id": "5sNV2X6LKPL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_wo_scaled.shape, y_train_wo.shape)\n",
        "print(X_test_wo_scaled.shape, y_test_wo.shape)\n",
        "\n",
        "# Check for NaNs or infinite values\n",
        "import numpy as np\n",
        "print(\"NaNs in X_train:\", np.isnan(X_train_wo_scaled).sum())\n",
        "print(\"NaNs in X_test:\", np.isnan(X_test_wo_scaled).sum())\n",
        "print(\"NaNs in y_train:\", y_train_wo.isna().sum())\n",
        "print(\"NaNs in y_test:\", y_test_wo.isna().sum())\n"
      ],
      "metadata": {
        "id": "mfdkSDBZNbNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lazypredict\n"
      ],
      "metadata": {
        "id": "CMXW6mPBMNqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Initialize LazyClassifier\n",
        "lazy_clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "\n",
        "# Fit and evaluate on scaled dataset\n",
        "models, predictions = lazy_clf.fit(X_train_wo_scaled, X_test_wo_scaled, y_train_wo, y_test_wo)\n",
        "\n",
        "# Display Top 10 Models\n",
        "print(\"ðŸ” Top Performing Models:\")\n",
        "print(models.head(10))\n"
      ],
      "metadata": {
        "id": "pnGUW_BEML-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_wo_scaled.shape, y_train_wo.shape)\n",
        "print(X_test_wo_scaled.shape, y_test_wo.shape)\n"
      ],
      "metadata": {
        "id": "TMEknVJVNtfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct split again from raw df_without_outliers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_wo = df_without_outliers.drop(\"Outcome\", axis=1)\n",
        "y_wo = df_without_outliers[\"Outcome\"]\n",
        "\n",
        "X_train_wo, X_test_wo, y_train_wo, y_test_wo = train_test_split(X_wo, y_wo, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_wo_scaled = scaler.fit_transform(X_train_wo)\n",
        "X_test_wo_scaled = scaler.transform(X_test_wo)\n",
        "\n",
        "# Confirm shapes now\n",
        "print(X_train_wo_scaled.shape, y_train_wo.shape)\n",
        "print(X_test_wo_scaled.shape, y_test_wo.shape)\n"
      ],
      "metadata": {
        "id": "SCh-G4qAN0sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "# Instantiate and run\n",
        "lazy_clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "models, predictions = lazy_clf.fit(X_train_wo_scaled, X_test_wo_scaled, y_train_wo, y_test_wo)\n",
        "\n",
        "# Show top models\n",
        "print(\"ðŸ” Top Performing Models:\")\n",
        "print(models.head(10))\n"
      ],
      "metadata": {
        "id": "ZWC9Bnl6N8AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step: Feature Importance Visualization"
      ],
      "metadata": {
        "id": "CkQS_e7BKfxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Get feature names\n",
        "features = X_train_wo.columns\n",
        "coefficients = lr_wo.coef_[0]\n",
        "\n",
        "# Create a DataFrame\n",
        "lr_importance = pd.DataFrame({'Feature': features, 'Importance': coefficients})\n",
        "lr_importance = lr_importance.sort_values(by='Importance', key=abs, ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(lr_importance['Feature'], lr_importance['Importance'])\n",
        "plt.title(\"ðŸ”¹ Logistic Regression - Feature Importance (Coefficients)\")\n",
        "plt.xlabel(\"Coefficient Value\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xDllRVolKhkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get importances\n",
        "rf_importance = pd.Series(rf_wo.feature_importances_, index=X_train_wo.columns)\n",
        "rf_importance = rf_importance.sort_values(ascending=True)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "rf_importance.plot(kind='barh')\n",
        "plt.title(\"ðŸ”¹ Random Forest - Feature Importance\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8jeJJXx7KmI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get importances\n",
        "xgb_importance = pd.Series(xgb_wo.feature_importances_, index=X_train_wo.columns)\n",
        "xgb_importance = xgb_importance.sort_values(ascending=True)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "xgb_importance.plot(kind='barh')\n",
        "plt.title(\"ðŸ”¹ XGBoost - Feature Importance\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_4PYSIryKqap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tune Top Models Using Optuna"
      ],
      "metadata": {
        "id": "8L72aGEROnNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Step 1: Define Optuna tuning function\n",
        "def tune_logistic(trial):\n",
        "    C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)\n",
        "    penalty = trial.suggest_categorical(\"penalty\", [\"l2\"])\n",
        "    solver = trial.suggest_categorical(\"solver\", [\"lbfgs\", \"liblinear\"])\n",
        "\n",
        "    model = LogisticRegression(C=C, penalty=penalty, solver=solver, max_iter=1000)\n",
        "    return cross_val_score(model, X_train_wo_scaled, y_train_wo, cv=5, scoring=\"roc_auc\").mean()\n",
        "\n",
        "# Step 2: Run the optimization\n",
        "study_lr = optuna.create_study(direction=\"maximize\")\n",
        "study_lr.optimize(tune_logistic, n_trials=20)\n",
        "\n",
        "# Step 3: Get best parameters and print them\n",
        "best_lr_params = study_lr.best_params\n",
        "print(\"âœ… Best Logistic Regression Parameters:\", best_lr_params)\n",
        "\n",
        "# Step 4: Train model using best parameters\n",
        "lr_best = LogisticRegression(**best_lr_params, max_iter=1000)\n",
        "lr_best.fit(X_train_wo_scaled, y_train_wo)\n",
        "\n",
        "# Step 5: Evaluate model\n",
        "y_pred_lr = lr_best.predict(X_test_wo_scaled)\n",
        "y_proba_lr = lr_best.predict_proba(X_test_wo_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nðŸ“Š Evaluation on Test Set (Logistic Regression):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_lr))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_wo, y_proba_lr))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_wo, y_pred_lr))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_lr))\n"
      ],
      "metadata": {
        "id": "XfxB6KWrOjbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "\n",
        "# ðŸ” Step 1: Define objective function for Optuna\n",
        "def tune_xgb(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "    }\n",
        "    model = XGBClassifier(**params, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "    return cross_val_score(model, X_train_wo_scaled, y_train_wo, cv=5, scoring=\"roc_auc\").mean()\n",
        "\n",
        "# ðŸ” Step 2: Run optimization\n",
        "study_xgb = optuna.create_study(direction=\"maximize\")\n",
        "study_xgb.optimize(tune_xgb, n_trials=20)\n",
        "\n",
        "# âœ… Step 3: Best parameters\n",
        "best_xgb_params = study_xgb.best_params\n",
        "print(\"âœ… Best XGB Parameters:\", best_xgb_params)\n",
        "\n",
        "# ðŸŽ¯ Step 4: Train model with best parameters\n",
        "xgb_best = XGBClassifier(**best_xgb_params, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "xgb_best.fit(X_train_wo_scaled, y_train_wo)\n",
        "\n",
        "# ðŸ§ª Step 5: Evaluation\n",
        "y_pred_xgb = xgb_best.predict(X_test_wo_scaled)\n",
        "y_proba_xgb = xgb_best.predict_proba(X_test_wo_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nðŸ“Š Evaluation on Test Set (XGBClassifier):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_xgb))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_wo, y_proba_xgb))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "P4dGMDGWOu5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "\n",
        "# ðŸ“Œ Objective function to optimize var_smoothing\n",
        "def objective(trial):\n",
        "    var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-12, 1e-6)\n",
        "    model = GaussianNB(var_smoothing=var_smoothing)\n",
        "\n",
        "    # 5-fold CV on training set\n",
        "    score = cross_val_score(model, X_train_wo_scaled, y_train_wo,\n",
        "                            scoring='roc_auc', cv=5).mean()\n",
        "    return score\n",
        "\n",
        "# ðŸ” Run optimization\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# ðŸ“Œ Best parameters and score\n",
        "print(\"âœ… Best Parameters:\", study.best_params)\n",
        "print(\"ðŸ” Best ROC-AUC Score:\", study.best_value)\n",
        "\n",
        "# ðŸŽ¯ Train with best params\n",
        "best_gnb = GaussianNB(var_smoothing=study.best_params['var_smoothing'])\n",
        "best_gnb.fit(X_train_wo_scaled, y_train_wo)\n",
        "\n",
        "# ðŸ§ª Predict and Evaluate\n",
        "y_pred_gnb = best_gnb.predict(X_test_wo_scaled)\n",
        "y_proba_gnb = best_gnb.predict_proba(X_test_wo_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nðŸ“Š Evaluation on Test Set:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_gnb))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_wo, y_proba_gnb))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_gnb))\n"
      ],
      "metadata": {
        "id": "ycCY7mDAPzsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Best estimators\n",
        "estimators = [\n",
        "    ('lr', LogisticRegression(C=0.1334, penalty='l2', solver='liblinear')),\n",
        "    ('xgb', XGBClassifier(**best_xgb, use_label_encoder=False, eval_metric='logloss')),\n",
        "    ('gnb', GaussianNB(var_smoothing=2.634962028799826e-07))\n",
        "]\n",
        "\n",
        "# Meta learner\n",
        "final_estimator = LogisticRegression()\n",
        "\n",
        "# Create StackingClassifier\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_estimator,\n",
        "    passthrough=True,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "stacking_clf.fit(X_train_wo_scaled, y_train_wo)\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "y_pred_stack = stacking_clf.predict(X_test_wo_scaled)\n",
        "y_proba_stack = stacking_clf.predict_proba(X_test_wo_scaled)[:, 1]\n",
        "\n",
        "print(\"\\nðŸ“Š Stacking Ensemble Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_stack))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_wo, y_proba_stack))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_wo, y_pred_stack))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_stack))\n"
      ],
      "metadata": {
        "id": "MUHQH-vtO26f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = cross_val_score(stacking_clf, X_train_wo_scaled, y_train_wo, cv=5, scoring='roc_auc')\n",
        "print(\"âœ… Mean ROC-AUC:\", cv_scores.mean())\n",
        "print(\"ðŸ“‰ Standard Deviation:\", cv_scores.std())\n"
      ],
      "metadata": {
        "id": "8i4iyxVpTHJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(stacking_clf, X_train_wo_scaled, y_train_wo, cv=skf, scoring='roc_auc')\n",
        "print(\"Stratified ROC-AUC Mean:\", cv_scores.mean())\n",
        "print(\"Stratified ROC-AUC Std Dev:\", cv_scores.std())\n"
      ],
      "metadata": {
        "id": "nG9WgxYxTsMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Fit your best model again\n",
        "stacking_clf.fit(X_train_wo, y_train_wo)\n",
        "\n",
        "# Permutation Importance on Test Set\n",
        "result = permutation_importance(stacking_clf, X_test_wo, y_test_wo, n_repeats=10, random_state=42, scoring='roc_auc')\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sorted_idx = result.importances_mean.argsort()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(sorted_idx)), result.importances_mean[sorted_idx])\n",
        "plt.yticks(range(len(sorted_idx)), np.array(X_test_wo.columns)[sorted_idx])\n",
        "plt.title(\"Permutation Feature Importance (ROC-AUC)\")\n",
        "plt.xlabel(\"Mean Importance\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5r-nNDJaULOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict probabilities and labels\n",
        "y_probs = stacking_clf.predict_proba(X_test_wo)[:, 1]\n",
        "y_preds = stacking_clf.predict(X_test_wo)\n",
        "\n",
        "# Create a DataFrame for debugging\n",
        "df_debug = pd.DataFrame({\n",
        "    \"True Label\": y_test_wo,\n",
        "    \"Predicted Label\": y_preds,\n",
        "    \"Prediction Confidence\": y_probs\n",
        "})\n",
        "\n",
        "# Filter misclassified samples\n",
        "misclassified = df_debug[df_debug[\"True Label\"] != df_debug[\"Predicted Label\"]]\n",
        "\n",
        "# Visualize misclassified samples' confidence\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(data=misclassified, x=\"Prediction Confidence\", hue=\"True Label\", multiple=\"stack\", kde=True, palette=\"Set2\")\n",
        "plt.title(\"Confidence Distribution for Misclassified Samples\")\n",
        "plt.xlabel(\"Predicted Probability\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LByMKL4gUezz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "\n",
        "# 1. Get FPR, TPR, thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test_wo, y_proba_xgb)\n",
        "\n",
        "# 2. Find optimal index where TPR - FPR is maximum\n",
        "optimal_idx = (tpr - fpr).argmax()\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"âœ… Optimal Threshold: {optimal_threshold:.2f}\")\n",
        "\n",
        "# 3. Apply the optimal threshold\n",
        "y_pred_opt = (y_proba_xgb >= optimal_threshold).astype(int)\n",
        "\n",
        "# 4. Evaluate\n",
        "print(\"\\nðŸ“Š Evaluation with Optimized Threshold:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_opt))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test_wo, y_proba_xgb))  # same as before\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_wo, y_pred_opt))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_opt))\n"
      ],
      "metadata": {
        "id": "rWwG90XHXeac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "# Get predicted probabilities from stacking ensemble\n",
        "y_proba_stack = stacking_clf.predict_proba(X_test_wo_scaled)[:, 1]\n",
        "\n",
        "# Find optimal threshold\n",
        "fpr, tpr, thresholds = roc_curve(y_test_wo, y_proba_stack)\n",
        "optimal_idx = (tpr - fpr).argmax()\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"Optimal Threshold for Stacking: {optimal_threshold:.2f}\")\n",
        "\n",
        "# Apply threshold to get predicted labels\n",
        "y_pred_stack_opt = (y_proba_stack >= optimal_threshold).astype(int)\n",
        "\n",
        "# Evaluate the optimized ensemble predictions\n",
        "print(\"\\nðŸ“Š Evaluation with Optimized Threshold (Stacking):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_stack_opt))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test_wo, y_proba_stack))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_wo, y_pred_stack_opt))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_stack_opt))\n"
      ],
      "metadata": {
        "id": "dXI96kSiYMaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "# Define base models with tuned parameters\n",
        "log_reg = LogisticRegression(C=0.1334, penalty='l2', solver='liblinear')\n",
        "xgb_clf = XGBClassifier(**best_xgb, use_label_encoder=False, eval_metric='logloss')\n",
        "gnb_clf = GaussianNB(var_smoothing=2.634962028799826e-07)\n",
        "\n",
        "# Create soft voting ensemble\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_reg), ('xgb', xgb_clf), ('gnb', gnb_clf)],\n",
        "    voting='soft',  # Use 'hard' for majority vote\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "voting_clf.fit(X_train_wo_scaled, y_train_wo)\n",
        "\n",
        "# Predict\n",
        "y_pred_vote = voting_clf.predict(X_test_wo_scaled)\n",
        "y_proba_vote = voting_clf.predict_proba(X_test_wo_scaled)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\nðŸ“Š Voting Ensemble Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_vote))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_wo, y_proba_vote))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_wo, y_pred_vote))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_vote))\n"
      ],
      "metadata": {
        "id": "B2iJKIRWZLwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Step 1: Get predicted probabilities\n",
        "y_proba_voting = voting_clf.predict_proba(X_test_wo_scaled)[:, 1]\n",
        "\n",
        "# Step 2: Find optimal threshold\n",
        "fpr, tpr, thresholds = roc_curve(y_test_wo, y_proba_voting)\n",
        "optimal_idx = (tpr - fpr).argmax()\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"Optimal Threshold: {optimal_threshold:.2f}\")\n",
        "\n",
        "# Step 3: Recalculate predictions with the new threshold\n",
        "y_pred_voting_opt = (y_proba_voting >= optimal_threshold).astype(int)\n",
        "\n",
        "# Step 4: Evaluate\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
        "\n",
        "print(\"\\nðŸ“Š Evaluation with Optimized Threshold (Voting Ensemble):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_wo, y_pred_voting_opt))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test_wo, y_proba_voting))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_wo, y_pred_voting_opt))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_wo, y_pred_voting_opt))\n"
      ],
      "metadata": {
        "id": "IWbDDSJUZ6Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(voting_clf, \"voting_ensemble_final.pkl\")\n"
      ],
      "metadata": {
        "id": "lhq6p4akaLVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the VotingClassifier\n",
        "joblib.dump(voting_clf, 'voting_model.pkl')\n",
        "\n",
        "# Save the scaler used during preprocessing (like StandardScaler or MinMaxScaler)\n",
        "joblib.dump(scaler_wo, 'scaler.pkl')\n"
      ],
      "metadata": {
        "id": "6Cxy8BnYatuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/naidk/Internship-Portfolio.git\n",
        "%cd Internship-Portfolio\n"
      ],
      "metadata": {
        "id": "S6E48SkjjxXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "B6LEwbKLjzau"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}